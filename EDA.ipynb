{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import ttest_ind, chi2_contingency\n",
    "from statsmodels.stats.power import TTestIndPower"
   ],
   "id": "634b96535ffe3253"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dtype_dict = {\n",
    "    'route_id': 'int16',\n",
    "    'bus_id': 'int32',\n",
    "    'stop_sequence': 'int16',\n",
    "    'arrival_delay': 'int16',\n",
    "    'dwell_time': 'uint16',\n",
    "    'travel_time_for_previous_section': 'uint16',\n",
    "    'scheduled_travel_time': 'uint16',\n",
    "    'upstream_stop_delay': 'int16',\n",
    "    'origin_delay': 'int16',\n",
    "    'previous_bus_delay': 'int16',\n",
    "    'previous_trip_travel_time': 'uint16',\n",
    "    'traffic_condition': 'float32',\n",
    "    'recurrent_delay': 'float32'\n",
    "}\n",
    "\n",
    "dummy_vars = [\n",
    "    'factor(weather)Light_Rain', 'factor(weather)Light_Snow', 'factor(weather)Normal',\n",
    "    'factor(weather)Rain', 'factor(weather)Snow', 'factor(temperature)Cold',\n",
    "    'factor(temperature)Extra_cold', 'factor(temperature)Normal', 'factor(day_of_week)weekday',\n",
    "    'factor(day_of_week)weekend', 'factor(time_of_day)Afternoon_peak',\n",
    "    'factor(time_of_day)Morning_peak', 'factor(time_of_day)Off-peak'\n",
    "]\n",
    "\n",
    "for var in dummy_vars:\n",
    "    dtype_dict[var] = 'uint8'"
   ],
   "id": "7a46756ce0e712ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data_path = 'data/Dataset-PT.csv'\n",
    "\n",
    "df = pd.read_csv(\n",
    "    data_path,\n",
    "    dtype=dtype_dict,\n",
    "    parse_dates=['Calendar_date'],\n",
    "    date_format='%Y%m%d'\n",
    ")\n",
    "\n",
    "numeric_cols = [\n",
    "    'arrival_delay', 'dwell_time', 'travel_time_for_previous_section',\n",
    "    'scheduled_travel_time', 'upstream_stop_delay', 'origin_delay',\n",
    "    'previous_bus_delay', 'previous_trip_travel_time', 'traffic_condition',\n",
    "    'recurrent_delay'\n",
    "]\n",
    "\n",
    "categorical_columns = ['weather', 'temperature', 'day_of_week', 'time_of_day']\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "# Validation of dummy variables\n",
    "for var in dummy_vars:\n",
    "    if not df[var].isin([0, 1]).all():\n",
    "        raise ValueError(f\"The variable {var} contains values other than 0 and 1\")"
   ],
   "id": "ea27e2cfead9eea1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.insert(1, 'day_of_week_num', df['Calendar_date'].dt.dayofweek)",
   "id": "164db714798e9911"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Calculate Statistics\n",
    "## Continuous Variables"
   ],
   "id": "f4d82f96f694db50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_continuous_statistics(df, cols):\n",
    "    # Create a dictionary to store results\n",
    "    stats_dict = {\n",
    "        'Variable': [],\n",
    "        'Mean': [],\n",
    "        'Median': [],\n",
    "        'Standard Deviation': [],\n",
    "        '95th Percentile': [],\n",
    "        'Maximum': []\n",
    "    }\n",
    "\n",
    "    # Calculate statistics for each column\n",
    "    for col in cols:\n",
    "        stats_dict['Variable'].append(col)\n",
    "        stats_dict['Mean'].append(df[col].mean())\n",
    "        stats_dict['Median'].append(df[col].median())\n",
    "        stats_dict['Standard Deviation'].append(df[col].std())\n",
    "        stats_dict['95th Percentile'].append(df[col].quantile(0.95))\n",
    "        stats_dict['Maximum'].append(df[col].max())\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    stats_df = pd.DataFrame(stats_dict)\n",
    "\n",
    "    # Print the table in markdown format\n",
    "    print(\"\\n### Descriptive Statistics for Continuous Variables ###\\n\")\n",
    "    print(stats_df.to_markdown(index=False))\n",
    "\n",
    "    return stats_df\n",
    "\n",
    "\n",
    "# Calculate descriptive statistics for continuous variables\n",
    "continuous_stats = calculate_continuous_statistics(df, numeric_cols)"
   ],
   "id": "cdb5fcf0b9bceab2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Categorical Variables",
   "id": "e5b2fa594bb40734"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_categorical_frequencies(df, cols):\n",
    "    print(\"\\n### Absolute and Relative Frequencies for Categorical Variables ###\\n\")\n",
    "\n",
    "    # Iterate over categorical columns to calculate frequencies\n",
    "    for col in cols:\n",
    "        absolute_freq = df[col].value_counts()\n",
    "        relative_freq = df[col].value_counts(normalize=True) * 100\n",
    "\n",
    "        # Combine absolute and relative frequencies into a single DataFrame\n",
    "        freq_df = pd.DataFrame({\n",
    "            'Absolute Frequency': absolute_freq,\n",
    "            'Relative Frequency (%)': relative_freq\n",
    "        }).reset_index().rename(columns={'index': col})\n",
    "\n",
    "        # Print the table in markdown format\n",
    "        print(f\"\\nFrequencies for {col}:\\n\")\n",
    "        print(freq_df.to_markdown(index=False))\n",
    "\n",
    "\n",
    "# Apply the function to all categorical variables\n",
    "categorical_vars = ['weather', 'temperature', 'day_of_week', 'time_of_day']\n",
    "calculate_categorical_frequencies(df, categorical_vars)"
   ],
   "id": "a36530a37ac2109f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Continuous Variables by Day of the Week",
   "id": "f30ef92ee7a53d33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_statistics_by_day_of_week(df, cols):\n",
    "    # Group by day_of_week_num (0=Monday, ..., 6=Sunday) and calculate statistics\n",
    "    grouped_stats = df.groupby('day_of_week_num')[cols].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "    # Return the DataFrame with grouped statistics\n",
    "    return grouped_stats\n",
    "\n",
    "\n",
    "# Calculate aggregated statistics by day of the week excluding certain variables\n",
    "continuous_stats_by_day = calculate_statistics_by_day_of_week(df, numeric_cols_filtered)"
   ],
   "id": "95fe20acf20c2599"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "continuous_stats_by_day",
   "id": "1dc7d5620e8d5fff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Continuous Variables by Stop Sequence\n",
    "def calculate_grouped_statistics(df, group_by_col, continuous_cols):\n",
    "    grouped_stats = df.groupby(group_by_col)[continuous_cols].agg(['mean', 'median', 'std'])\n",
    "    print(f\"\\n### Aggregated Statistics by {group_by_col} ###\\n\")\n",
    "    return grouped_stats\n",
    "\n",
    "\n",
    "# Aggregated statistics by stop_sequence\n",
    "stats_by_stop = calculate_grouped_statistics(df, 'stop_sequence', numeric_cols)"
   ],
   "id": "7e8190a14c2912bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "stats_by_stop",
   "id": "a2fca3c46a8cec44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Categorical Variables by Day of the Week\n",
    "def calculate_categorical_frequencies_by_group(df, group_by_col, categorical_cols):\n",
    "    combined_freq_dict = {}\n",
    "\n",
    "    # Calculate frequencies for each categorical variable by the specified group\n",
    "    for col in categorical_cols:\n",
    "        # Absolute frequency by group\n",
    "        abs_freq = df.groupby(group_by_col)[col].value_counts().unstack().fillna(0)\n",
    "\n",
    "        # Relative frequency by group (proportion)\n",
    "        rel_freq = abs_freq.div(abs_freq.sum(axis=1), axis=0) * 100\n",
    "\n",
    "        # Combine absolute and relative frequencies into a single DataFrame\n",
    "        combined_freq = pd.concat([rel_freq], axis=1, keys=['(%)'])\n",
    "\n",
    "        # Rearrange columns so that relative frequencies are next to absolute frequencies\n",
    "        combined_freq.columns = [f'{lvl1}_{lvl2}' for lvl1, lvl2 in combined_freq.columns]\n",
    "\n",
    "        combined_freq_dict[col] = combined_freq\n",
    "\n",
    "    return combined_freq_dict\n",
    "\n",
    "\n",
    "# Define the categorical variables for analysis\n",
    "categorical_vars = ['weather', 'temperature', 'day_of_week', 'time_of_day']\n",
    "\n",
    "# Calculate frequencies for categorical variables by day of the week\n",
    "frequencies_by_day = calculate_categorical_frequencies_by_group(df, 'day_of_week_num', categorical_vars)\n",
    "\n",
    "# Display results for a categorical variable as an example\n",
    "for col, freqs in frequencies_by_day.items():\n",
    "    print(f\"\\n### Combined Frequencies for {col} ###\\n\")\n",
    "    print(freqs)"
   ],
   "id": "2ba271c9c41afe59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Categorical Variables by Stop Sequence\n",
    "def calculate_categorical_frequencies_by_stop_sequence(df, categorical_cols):\n",
    "    combined_freq_dict = {}\n",
    "\n",
    "    # Calculate frequencies for each categorical variable by stop_sequence\n",
    "    for col in categorical_cols:\n",
    "        # Absolute frequency by group (stop_sequence)\n",
    "        abs_freq = df.groupby('stop_sequence')[col].value_counts().unstack().fillna(0)\n",
    "\n",
    "        # Relative frequency by group (proportion)\n",
    "        rel_freq = abs_freq.div(abs_freq.sum(axis=1), axis=0) * 100\n",
    "\n",
    "        # Create DataFrame with only relative frequencies\n",
    "        combined_freq = pd.concat([rel_freq], axis=1, keys=['(%)'])\n",
    "\n",
    "        # Rename columns to reflect categories\n",
    "        combined_freq.columns = [f'{lvl1}_{lvl2}' for lvl1, lvl2 in combined_freq.columns]\n",
    "\n",
    "        combined_freq_dict[col] = combined_freq\n",
    "\n",
    "    return combined_freq_dict"
   ],
   "id": "47479f3682e2ebe6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate relative frequencies for categorical variables by stop_sequence\n",
    "frequencies_by_stop_sequence = calculate_categorical_frequencies_by_stop_sequence(df, categorical_vars)\n",
    "\n",
    "# Display results for a categorical variable as an example\n",
    "for col, freqs in frequencies_by_stop_sequence.items():\n",
    "    print(f\"\\n### Relative Frequencies for {col} by stop_sequence ###\\n\")\n",
    "    print(freqs)\n",
    "\n",
    "# Plots Mean and Standard Deviation\n",
    "## Continuous Variables vs. Day of the Week\n",
    "exclude_vars = ['travel_time_for_previous_section', 'scheduled_travel_time', 'origin_delay']\n",
    "\n",
    "# Filter numeric variables excluding the specified ones\n",
    "numeric_cols_filtered = [col for col in numeric_cols if col not in exclude_vars]\n",
    "\n",
    "\n",
    "def calculate_and_plot_by_day(df, cols, group_by_col='day_of_week_num', exclude=None):\n",
    "    # Exclude variables if the exclude list is provided\n",
    "    if exclude is not None:\n",
    "        cols = [col for col in cols if col not in exclude]\n",
    "\n",
    "    # Group by day_of_week_num and calculate statistics\n",
    "    grouped_stats = df.groupby(group_by_col)[cols].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "    # Calculate the number of rows needed for the grid (2 columns)\n",
    "    num_cols = 2\n",
    "    num_rows = (len(cols) + 1) // num_cols\n",
    "\n",
    "    # Set up the figure size\n",
    "    plt.figure(figsize=(15, num_rows * 5))\n",
    "\n",
    "    # Plot each continuous variable\n",
    "    for i, col in enumerate(cols, 1):\n",
    "        plt.subplot(num_rows, num_cols, i)\n",
    "\n",
    "        # Extract mean and standard deviation\n",
    "        x = grouped_stats[group_by_col]\n",
    "        y_mean = grouped_stats[(col, 'mean')]\n",
    "        y_std = grouped_stats[(col, 'std')]\n",
    "\n",
    "        # Plot mean and standard deviation band\n",
    "        plt.plot(x, y_mean, label='Mean', color='blue', marker='o')\n",
    "        plt.fill_between(x, y_mean - y_std, y_mean + y_std, color='blue', alpha=0.2, label='Standard Deviation')\n",
    "\n",
    "        # Customize the plot\n",
    "        plt.title(f'Mean and Standard Deviation of {col} by {group_by_col}')\n",
    "        plt.xlabel('Day of the Week (0=Monday, 6=Sunday)')\n",
    "        plt.ylabel(col.capitalize())\n",
    "        plt.xticks(ticks=range(7), labels=[\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"])  # Day labels\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Return grouped statistics\n",
    "    return grouped_stats\n"
   ],
   "id": "176318b8002f7f0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Apply the function to calculate and plot statistics by day of the week excluding certain variables\n",
    "stats_by_day = calculate_and_plot_by_day(df, numeric_cols, exclude=exclude_vars)\n",
    "## Categorical Variables vs. Stop Sequence\n",
    "exclude_categorical_vars = ['day_of_week']\n",
    "\n",
    "\n",
    "def plot_categorical_frequencies_by_day(df, categorical_cols, group_by_col='day_of_week_num'):\n",
    "    # Calculate relative frequencies by group (day of the week)\n",
    "    for col in categorical_cols:\n",
    "        # Calculate absolute and then relative frequencies\n",
    "        abs_freq = df.groupby(group_by_col)[col].value_counts().unstack().fillna(0)\n",
    "        rel_freq = abs_freq.div(abs_freq.sum(axis=1), axis=0) * 100\n",
    "\n",
    "        # Set up the figure size\n",
    "        plt.figure(figsize=(15, 7))\n",
    "\n",
    "        # Create plot for each categorical variable\n",
    "        for category in rel_freq.columns:\n",
    "            plt.plot(rel_freq.index, rel_freq[category], marker='o', label=f'{col}: {category}')\n",
    "\n",
    "        # Customize the plot\n",
    "        plt.title(f'Relative Frequencies of {col} by {group_by_col}')\n",
    "        plt.xlabel('Day of the Week (0=Monday, 6=Sunday)')\n",
    "        plt.ylabel('Relative Frequency (%)')\n",
    "        plt.xticks(ticks=range(7), labels=[\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"])  # Day labels\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Plot relative frequencies of categorical variables by day of the week\n",
    "plot_categorical_frequencies_by_day(df, categorical_vars_filtered)\n",
    "## Categorical Variables vs. Stop Sequence\n",
    "exclude_categorical_vars = ['day_of_week']\n",
    "\n",
    "# Filter categorical variables excluding the specified ones\n",
    "categorical_vars_filtered = [col for col in categorical_vars if col not in exclude_categorical_vars]\n"
   ],
   "id": "b44846f07a088584"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_categorical_frequencies_by_stop_sequence(df, categorical_cols, group_by_col='stop_sequence'):\n",
    "    # Calculate relative frequencies by group (stop_sequence)\n",
    "    for col in categorical_cols:\n",
    "        # Calculate absolute and then relative frequencies\n",
    "        abs_freq = df.groupby(group_by_col)[col].value_counts().unstack().fillna(0)\n",
    "        rel_freq = abs_freq.div(abs_freq.sum(axis=1), axis=0) * 100\n",
    "\n",
    "        # Set up the figure size\n",
    "        plt.figure(figsize=(15, 7))\n",
    "\n",
    "        # Create plot for each categorical variable\n",
    "        for category in rel_freq.columns:\n",
    "            plt.plot(rel_freq.index, rel_freq[category], marker='o', label=f'{col}: {category}')\n",
    "\n",
    "        # Customize the plot\n",
    "        plt.title(f'Relative Frequencies of {col} by {group_by_col}')\n",
    "        plt.xlabel(group_by_col.capitalize())\n",
    "        plt.ylabel('Relative Frequency (%)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Plot relative frequencies of categorical variables by stop_sequence\n",
    "plot_categorical_frequencies_by_stop_sequence(df, categorical_vars_filtered)"
   ],
   "id": "29fdf76495647a06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Histogram Plots\n",
    "## Histograms of Continuous Variables\n",
    "def plot_histogram(df, cols, bins=30):\n",
    "    rows = (len(cols) + 1) // 2\n",
    "    plt.figure(figsize=(15, rows * 4))\n",
    "    for i, col in enumerate(cols, 1):\n",
    "        plt.subplot(rows, 2, i)\n",
    "        sns.histplot(df[col], kde=True, bins=bins)\n",
    "        plt.title(f'Histogram of {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "numeric_cols = ['arrival_delay', 'dwell_time', 'travel_time_for_previous_section',\n",
    "                'scheduled_travel_time', 'upstream_stop_delay', 'origin_delay',\n",
    "                'previous_bus_delay', 'previous_trip_travel_time', 'traffic_condition',\n",
    "                'recurrent_delay']\n",
    "plot_histogram(df, numeric_cols)"
   ],
   "id": "acd8f05ea6a156cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Crossed Histograms between Continuous Variables\n",
    "def plot_crossed_histograms_grid(df, pairs, bins=30):\n",
    "    num_pairs = len(pairs)\n",
    "    num_cols = 2\n",
    "    num_rows = (num_pairs + 1) // num_cols\n",
    "\n",
    "    plt.figure(figsize=(15, num_rows * 5))\n",
    "\n",
    "    # Plot each pair of continuous variables in a grid\n",
    "    for i, (col_x, col_y) in enumerate(pairs, 1):\n",
    "        plt.subplot(num_rows, num_cols, i)\n",
    "\n",
    "        # Create scatter plot (2D histogram)\n",
    "        sns.histplot(data=df, x=col_x, y=col_y, bins=bins, pthresh=.1, cmap=\"viridis\")\n",
    "\n",
    "        # Customize the plot\n",
    "        plt.title(f'{col_y} vs {col_x}')\n",
    "        plt.xlabel(col_x.capitalize())\n",
    "        plt.ylabel(col_y.capitalize())\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Suggest some interesting combinations of continuous variables\n",
    "continuous_pairs = [\n",
    "    ('arrival_delay', 'previous_bus_delay'),\n",
    "    ('arrival_delay', 'upstream_stop_delay'),\n",
    "    ('travel_time_for_previous_section', 'scheduled_travel_time'),\n",
    "    ('previous_trip_travel_time', 'arrival_delay'),\n",
    "    ('traffic_condition', 'recurrent_delay')\n",
    "]\n",
    "\n",
    "# Plot crossed combinations of continuous variables in a grid\n",
    "plot_crossed_histograms_grid(df, continuous_pairs)\n"
   ],
   "id": "361d67aaf12fca3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Crossed Histograms between Continuous and Categorical Variables\n",
    "def plot_continuous_vs_categorical_grid(df, continuous_vars, categorical_vars):\n",
    "    # Calculate the total number of plots and arrange in 2 columns\n",
    "    num_plots = len(continuous_vars) * len(categorical_vars)\n",
    "    num_cols = 2\n",
    "    num_rows = (num_plots + 1) // num_cols\n",
    "\n",
    "    plt.figure(figsize=(15, num_rows * 5))\n",
    "\n",
    "    # Index for subplot position\n",
    "    plot_idx = 1\n",
    "\n",
    "    # Iterate over each combination of continuous and categorical variable\n",
    "    for col_cont in continuous_vars:\n",
    "        for col_cat in categorical_vars:\n",
    "            plt.subplot(num_rows, num_cols, plot_idx)\n",
    "            plot_idx += 1\n",
    "\n",
    "            # Create boxplot of the continuous variable by the categorical variable\n",
    "            sns.boxplot(x=col_cat, y=col_cont, data=df)\n",
    "\n",
    "            # Customize the plot\n",
    "            plt.title(f'{col_cont.capitalize()} by {col_cat.capitalize()}')\n",
    "            plt.xlabel(col_cat.capitalize())\n",
    "            plt.ylabel(col_cont.capitalize())\n",
    "            plt.xticks(rotation=45)  # Rotate labels if necessary\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define continuous and categorical variables to cross\n",
    "interesting_continuous_vars = ['arrival_delay', 'travel_time_for_previous_section', 'scheduled_travel_time',\n",
    "                               'traffic_condition']\n",
    "interesting_categorical_vars = ['weather', 'temperature', 'time_of_day', 'day_of_week_num']\n",
    "\n",
    "# Plot combinations of continuous and categorical variables in a grid\n",
    "plot_continuous_vs_categorical_grid(df, interesting_continuous_vars, interesting_categorical_vars)"
   ],
   "id": "cfaff9e5b3d552bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Bar Charts\n",
    "## Bar Charts of Categorical Variables\n",
    "def plot_categorical_bars(df, categorical_cols):\n",
    "    rows = (len(categorical_cols) + 1) // 2\n",
    "    plt.figure(figsize=(15, rows * 5))\n",
    "\n",
    "    for i, col in enumerate(categorical_cols, 1):\n",
    "        plt.subplot(rows, 2, i)\n",
    "        # Count the frequency of each category\n",
    "        value_counts = df[col].value_counts()\n",
    "\n",
    "        # Create bar plot\n",
    "        sns.barplot(x=value_counts.index, y=value_counts.values)\n",
    "\n",
    "        # Customize the plot\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel(col.capitalize())\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.xticks(rotation=45)  # Rotate labels if necessary\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define categorical variables to plot\n",
    "categorical_vars = ['weather', 'temperature', 'time_of_day']\n",
    "\n",
    "# Plot frequency distribution of categorical variables\n",
    "plot_categorical_bars(df, categorical_vars)\n"
   ],
   "id": "540baad924363b10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Box Plots\n",
    "## Box Plots of Continuous Variables\n",
    "def plot_boxplots_continuous(df, continuous_vars):\n",
    "    # Calculate the total number of variables and arrange in 2 columns\n",
    "    num_vars = len(continuous_vars)\n",
    "    num_cols = 2\n",
    "    num_rows = (num_vars + 1) // num_cols\n",
    "\n",
    "    plt.figure(figsize=(15, num_rows * 5))\n",
    "\n",
    "    # Plot each continuous variable\n",
    "    for i, col in enumerate(continuous_vars, 1):\n",
    "        plt.subplot(num_rows, num_cols, i)\n",
    "\n",
    "        # Create boxplot of the continuous variable\n",
    "        sns.boxplot(data=df, y=col)\n",
    "\n",
    "        # Customize the plot\n",
    "        plt.title(f'Boxplot of {col}')\n",
    "        plt.ylabel(col.capitalize())\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define continuous variables to plot\n",
    "continuous_vars = ['arrival_delay', 'travel_time_for_previous_section', 'scheduled_travel_time',\n",
    "                   'traffic_condition', 'recurrent_delay', 'upstream_stop_delay', 'dwell_time']\n",
    "\n",
    "# Plot boxplots of the continuous variables\n",
    "plot_boxplots_continuous(df, continuous_vars)"
   ],
   "id": "5385483edda3c67c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Outliers\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def remove_multiple_outliers(df, cols, threshold=3):\n",
    "    initial_rows = df.shape[0]  # Initial number of rows\n",
    "    total_removed = pd.Series(0, index=cols)  # Series to count rows removed per column\n",
    "\n",
    "    # Iterate over each column and remove outliers\n",
    "    for col in cols:\n",
    "        z_scores = stats.zscore(df[col])\n",
    "        abs_z_scores = np.abs(z_scores)\n",
    "        is_not_outlier = abs_z_scores <= threshold\n",
    "\n",
    "        # Count removed rows\n",
    "        total_removed[col] = initial_rows - is_not_outlier.sum()\n",
    "\n",
    "        # Filter the DataFrame for the current column\n",
    "        df = df[is_not_outlier]\n",
    "\n",
    "    # Calculate the total number of rows after removing outliers\n",
    "    final_rows = df.shape[0]\n",
    "    total_removed_absolute = initial_rows - final_rows\n",
    "\n",
    "    # Calculate removal percentages\n",
    "    removal_percentage = (total_removed / initial_rows) * 100\n",
    "    total_removal_percentage = (total_removed_absolute / initial_rows) * 100\n",
    "\n",
    "    # Print removal statistics per column\n",
    "    print(\"\\n### Outlier Removal Statistics ###\\n\")\n",
    "    print(f\"Initial number of rows: {initial_rows}\")\n",
    "    print(f\"Final number of rows: {final_rows}\")\n",
    "    print(f\"Total rows removed: {total_removed_absolute} ({total_removal_percentage:.2f}%)\")\n",
    "    print(\"\\nRows removed per column:\")\n",
    "    for col in cols:\n",
    "        print(f\" - {col}: {total_removed[col]} removed ({removal_percentage[col]:.2f}%)\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Define continuous variables for which to remove outliers\n",
    "'''\n",
    "continuous_vars = ['arrival_delay', 'travel_time_for_previous_section', 'scheduled_travel_time',\n",
    "                   'traffic_condition', 'recurrent_delay', 'upstream_stop_delay', 'dwell_time']\n",
    "'''\n",
    "continuous_vars = ['arrival_delay', 'dwell_time']\n",
    "\n",
    "# Remove outliers from multiple columns\n",
    "df_cleaned_multiple = remove_multiple_outliers(df, continuous_vars)\n",
    "# Export the cleaned DataFrame as a CSV file\n",
    "output_path = \"data/Dataset-PT_no_sample.csv\"\n",
    "df_cleaned_multiple.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"The DataFrame has been successfully exported to {output_path}\")"
   ],
   "id": "eb9a37a8dcf36f94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Sub Sampling\n",
    "## Stratified Sampling\n",
    "df_sorted = df_cleaned_multiple.sort_values('Calendar_date')\n",
    "df_time_sampled = df_sorted.iloc[::10, :].reset_index(drop=True)\n",
    "\n",
    "df_train, df_stratified = train_test_split(\n",
    "    df_time_sampled,\n",
    "    test_size=0.1,\n",
    "    stratify=df_time_sampled['day_of_week'],\n",
    "    random_state=42\n",
    ")\n",
    "# Export the cleaned DataFrame as a CSV file\n",
    "output_path = \"data/Dataset-PT_stratified.csv\"\n",
    "df_stratified.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"The DataFrame has been successfully exported to {output_path}\")"
   ],
   "id": "2268fdf269d8b2ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## KMeans Sampling\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def kmeans_subsampling(df, columns_to_exclude, n_clusters=10, sample_percentage=0.01):\n",
    "    # Exclude specific columns\n",
    "    df_filtered = df.drop(columns=columns_to_exclude)\n",
    "\n",
    "    # Separate features (X) and target variable (y)\n",
    "    X = df_filtered.drop(['arrival_delay'], axis=1)\n",
    "    y = df_filtered['arrival_delay']\n",
    "\n",
    "    # Determine the total number of rows to calculate sample size per cluster\n",
    "    total_rows = X.shape[0]\n",
    "\n",
    "    # Calculate the number of samples per cluster as 1% of the total rows\n",
    "    samples_per_cluster = int(sample_percentage * total_rows / n_clusters)\n",
    "\n",
    "    # Apply KMeans clustering using the filtered columns\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X)\n",
    "    df['cluster'] = clusters\n",
    "\n",
    "    # Sample a percentage from each cluster, allowing replacement if necessary\n",
    "    df_kmeans_sampled = df.groupby('cluster', group_keys=False).apply(\n",
    "        lambda x: x.sample(\n",
    "            n=samples_per_cluster,  # Always take the calculated size\n",
    "            replace=True,  # Allow replacement if there are fewer rows in the cluster\n",
    "            random_state=42\n",
    "        )\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return df_kmeans_sampled\n",
    "\n",
    "\n",
    "# Define columns to exclude for KMeans\n",
    "columns_to_drop = ['Calendar_date', 'route_id', 'bus_id', 'weather', 'temperature', 'day_of_week', 'time_of_day']\n",
    "\n",
    "# Apply KMeans for sampling based on 1% of total rows\n",
    "df_kmeans_sampled = kmeans_subsampling(df_cleaned_multiple, columns_to_drop)\n",
    "df_kmeans_sampled = df_kmeans_sampled.drop(columns=['cluster'])\n",
    "# Export the cleaned DataFrame as a CSV file\n",
    "output_path = \"data/Dataset-PT_KMeans.csv\"\n",
    "df_kmeans_sampled.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"The DataFrame has been successfully exported to {output_path}\")"
   ],
   "id": "38218b159c9c5161"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plots Original vs. KMeans Distribution\n",
    "def plot_distributions(df_original, df_sampled, cols):\n",
    "    for col in cols:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(df_original[col], color='blue', label='Original', kde=True, stat=\"density\")\n",
    "        sns.histplot(df_sampled[col], color='orange', label='Sampled', kde=True, stat=\"density\")\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot_distributions(df, df_kmeans_sampled, numeric_cols)\n"
   ],
   "id": "f2e712a5e86fac03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Statistical Comparisons\n",
    "## Statistical Comparison\n",
    "def compare_statistics(df_original, df_sampled, numeric_cols, threshold=0.05):\n",
    "    original_summary = df_original[numeric_cols].describe()\n",
    "    sampled_summary = df_sampled[numeric_cols].describe()\n",
    "\n",
    "    print(\"\\n### Statistics Comparison ###\")\n",
    "    for col in numeric_cols:\n",
    "        mean_diff = np.abs(original_summary.loc['mean', col] - sampled_summary.loc['mean', col]) / original_summary.loc[\n",
    "            'mean', col]\n",
    "        print(f\"{col} - Mean Difference: {mean_diff:.2%} (Threshold: {threshold * 100}%)\")\n",
    "\n",
    "        # Other comparisons of medians and standard deviations can be added here\n",
    "\n",
    "\n",
    "compare_statistics(df, df_stratified, numeric_cols)\n",
    "compare_statistics(df, df_kmeans_sampled, numeric_cols)"
   ],
   "id": "5ac188b5bacd199d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## T-test\n",
    "def perform_t_test(df_original, df_sampled, cols, alpha=0.05):\n",
    "    print(\"\\n### T-tests ###\")\n",
    "    for col in cols:\n",
    "        t_stat, p_value = ttest_ind(df_original[col], df_sampled[col])\n",
    "        result = \"Significant\" if p_value < alpha else \"Not Significant\"\n",
    "        print(f\"{col} - p-value: {p_value:.3f} ({result})\")\n",
    "\n",
    "\n",
    "perform_t_test(df, df_stratified, numeric_cols)\n",
    "perform_t_test(df, df_kmeans_sampled, numeric_cols)\n"
   ],
   "id": "bd02b2dc5fda1fa9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Chi-square\n",
    "def perform_chi_square_test(df_original, df_sampled, dummy_vars, alpha=0.05):\n",
    "    print(\"\\n### Chi-Square Tests ###\")\n",
    "    for var in dummy_vars:\n",
    "        contingency_table = pd.crosstab(df_original[var], df_sampled[var])\n",
    "        _, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "        result = \"Significant\" if p_value < alpha else \"Not Significant\"\n",
    "        print(f\"{var} - p-value: {p_value:.3f} ({result})\")\n",
    "\n",
    "\n",
    "perform_chi_square_test(df, df_stratified, dummy_vars)\n",
    "perform_chi_square_test(df, df_kmeans_sampled, dummy_vars)"
   ],
   "id": "6ac5915f1f9b0b1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Creating Scenario 2 and Scenario 3\n",
    "## Scenario 2\n",
    "columns_to_drop = ['travel_time_for_previous_section', 'recurrent_delay', 'previous_trip_travel_time']\n",
    "s2_nosample = df_cleaned_multiple.drop(columns=columns_to_drop)\n",
    "s2_stratified = df_stratified.drop(columns=columns_to_drop)\n",
    "s2_kMeans = df_kmeans_sampled.drop(columns=columns_to_drop)\n",
    "output_path = \"data/s2_no_sample.csv\"\n",
    "s2_nosample.to_csv(output_path, index=False)\n",
    "print(f\"The DataFrame has been successfully exported to {output_path}\")\n",
    "output_path = \"data/s2_stratified.csv\"\n",
    "s2_stratified.to_csv(output_path, index=False)\n",
    "print(f\"The DataFrame has been successfully exported to {output_path}\")\n",
    "output_path = \"data/s2_KMeans.csv\"\n",
    "s2_kMeans.to_csv(output_path, index=False)\n",
    "print(f\"The DataFrame has been successfully exported to {output_path}\")"
   ],
   "id": "6e49e053f8f734fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Scenario 3\n",
    "columns_to_drop = ['travel_time_for_previous_section', 'recurrent_delay', 'previous_trip_travel_time', 'dwell_time',\n",
    "                   'traffic_condition']\n",
    "s3_nosample = df_cleaned_multiple.drop(columns=columns_to_drop)\n",
    "s3_stratified = df_stratified.drop(columns=columns_to_drop)\n",
    "s3_kMeans = df_kmeans_sampled.drop(columns=columns_to_drop)\n",
    "output_path = \"data/s3_no_sample.csv\"\n",
    "s3_nosample.to_csv(output_path, index=False)\n",
    "print(f\"The DataFrame has been successfully exported to {output_path}\")\n",
    "output_path = \"data/s3_stratified.csv\"\n",
    "s3_stratified.to_csv(output_path, index=False)\n",
    "print(f\"The DataFrame has been successfully exported to {output_path}\")\n",
    "output_path = \"data/s3_KMeans.csv\"\n",
    "s3_kMeans.to_csv(output_path, index=False)\n",
    "print(f\"The DataFrame has been successfully exported to {output_path}\")"
   ],
   "id": "5c7f49a10a69394c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
